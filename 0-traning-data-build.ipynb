{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the folder path containing .shp files\n",
    "folder_path = './data/useful-poi'\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through all .shp files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".shp\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read the .shp file using geopandas\n",
    "        gdf = gpd.read_file(file_path)\n",
    "        # Convert to Pandas DataFrame and remove the geometry column\n",
    "        df = pd.DataFrame(gdf.drop(columns='geometry'))\n",
    "        # Use the filename without extension as the dictionary key\n",
    "        dataframes[os.path.splitext(filename)[0]] = df\n",
    "\n",
    "# Assign the dataframes to specific variables\n",
    "landuse = dataframes['gis_osm_landuse_a_free_1']\n",
    "buildings = dataframes['gis_osm_buildings_a_free_1']\n",
    "places = dataframes['gis_osm_places_free_1']\n",
    "pois = dataframes['gis_osm_pois_free_1']\n",
    "roads = dataframes['gis_osm_roads_free_1']\n",
    "transport = dataframes['gis_osm_transport_free_1']\n",
    "\n",
    "# Extract columns we need\n",
    "landuse_subset = landuse[['fclass', 'osm_id', 'name']]\n",
    "buildings_subset = buildings[['fclass', 'osm_id', 'name']]\n",
    "places_subset = places[['fclass', 'osm_id', 'name']]\n",
    "pois_subset = pois[['fclass', 'osm_id', 'name']]\n",
    "roads_subset = roads[['fclass', 'osm_id', 'name']]\n",
    "transport_subset = transport[['fclass', 'osm_id', 'name']]\n",
    "\n",
    "# Merge all DataFrames\n",
    "frames = [landuse_subset, buildings_subset, places_subset, pois_subset, roads_subset, transport_subset]\n",
    "combined_osm = pd.concat(frames)\n",
    "\n",
    "# Drop rows with missing 'name' values\n",
    "combined_osm = combined_osm.dropna(subset=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_osm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    # Remove or replace characters that may cause formatting issues\n",
    "    for char in ['\"', \"#\", \"$\", \"%\", \" - \", \",\", \"`\", \"(\", \")\", \"*\", \"+\", \".\", \"?\"]:\n",
    "        name = name.replace(char, \"\")\n",
    "    return name.strip()\n",
    "\n",
    "combined_osm['location_name'] = combined_osm['name'].apply(clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_osm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_osm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the 'fclass' field\n",
    "filtered_osm = combined_osm[~combined_osm['fclass'].isin(['convince', 'clothes', 'supermarket', 'pharmacy', 'fast_food', 'cafe', 'bar'])]\n",
    "\n",
    "# Filter rows where 'fclass' is 'building' and the 'name' field contains only one word\n",
    "filtered_osm = filtered_osm[~((filtered_osm['fclass'] == 'building') & (filtered_osm['name'].str.split().str.len() == 1))]\n",
    "\n",
    "# Filter rows where the 'name' field contains only numbers\n",
    "filtered_osm = filtered_osm[~filtered_osm['name'].str.match(r'^\\d+$')]\n",
    "\n",
    "location_set = set(filtered_osm['location_name'])\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def match_longest_place_names(text, location_set):\n",
    "    tokens = word_tokenize(text)\n",
    "    matched_names = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens), i, -1):\n",
    "            candidate = ' '.join(tokens[i:j])\n",
    "            if candidate in location_set and not any(candidate in name for name in matched_names):\n",
    "                matched_names.append(candidate)\n",
    "                break\n",
    "    return matched_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = '''\n",
    "Finsbury Park is a friendly melting pot community composed of Turkish, French, Spanish, Middle Eastern,\n",
    "Irish and English families. \n",
    "We have a wonderful variety of international restaurants directly under us on Stroud Green Road.\n",
    "And there are many shops and large Tescos supermarket right next door. \n",
    "But you can also venture up to Crouch End and along Greens Lanes where there will endless choice \n",
    "of Turkish and Middle Eastern cuisines.\n",
    "'''\n",
    "\n",
    "match_longest_place_names(tt, location_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighborhood_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters except for alphanumeric, spaces, periods, and commas. Note that we included \\. and , in the exclusion set, so they will not be removed\n",
    "    text = re.sub(r'[^\\w\\s.,]', '', text)\n",
    "    # Remove numbers (if needed)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Load the CSV file\n",
    "train_data = pd.read_csv('listings_d.csv')\n",
    "\n",
    "# Get the 'neighborhood_overview' column as a list\n",
    "text_data = train_data['neighborhood_overview'].tolist()\n",
    "\n",
    "# Clean the text data\n",
    "ok_data = []\n",
    "for i in text_data:\n",
    "    try:\n",
    "        ok_data.append(clean_text(i))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Print the length of the cleaned data\n",
    "len(ok_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_len = 0\n",
    "ok_data = []\n",
    "for i in text_Data:\n",
    "    try:\n",
    "        sum_len += len(i.split(' '))\n",
    "        ok_data.append(i)\n",
    "    except:\n",
    "        pass\n",
    "print(sum_len/len(ok_data), len(ok_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "texts = []\n",
    "labels = []\n",
    "for tt in tqdm(ok_data):\n",
    "    locationsi = match_longest_place_names(tt, location_set)\n",
    "    if locationsi:\n",
    "        texts.append(tt)\n",
    "        labels.append(locationsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in zip(texts, labels):\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Labels:\", label)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('texts.pkl', 'wb') as f:\n",
    "    pickle.dump(texts, f)\n",
    "with open('labels.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Data Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('../bert-base-uncased')\n",
    "\n",
    "def tokenize_and_label(text, location_list, len_tokens):\n",
    "    labels = ['O'] * len_tokens\n",
    "    for location in location_list:\n",
    "        # Find the start index of the location in the tokens\n",
    "        start_idx = text.find(location)\n",
    "        if start_idx != -1:\n",
    "            # Convert start index to token index\n",
    "            start_token_idx = len(tokenizer.tokenize(text[:start_idx]))\n",
    "            # print(text, start_idx, text[:start_idx], location, start_token_idx)\n",
    "            if labels[start_token_idx] == 'O':\n",
    "                # Mark the start of the location with 'B-LOC'\n",
    "                labels[start_token_idx] = 'B-LOC'\n",
    "                # Mark the subsequent tokens of the location with 'I-LOC'\n",
    "                for i in range(start_token_idx + 1, start_token_idx + len(tokenizer.tokenize(location))):\n",
    "                    labels[i] = 'I-LOC'\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 'Finsbury Park is a friendly melting pot community composed of Turkish, French, Spanish, Middle Eastern, Irish and English families. <br />We have a wonderful variety of international restaurants directly under us on Stroud Green Road. And there are many shops and large Tescos supermarket right next door. <br /><br />But you can also venture up to Crouch End and along Greens Lanes where there will endless choice of Turkish and Middle Eastern cuisines.'\n",
    "locationsi = match_longest_place_names(tt, location_set)\n",
    "tokens = tokenizer.tokenize(tt)\n",
    "tokenize_and_label(tt, locationsi, len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "all_label_bio = []\n",
    "for text, location in tqdm.tqdm(zip(texts, labels)):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    labelsbio = tokenize_and_label(text, location, len(tokens))\n",
    "    all_label_bio.append(labelsbio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts), len(all_label_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_label_bio.pkl', 'wb') as f:\n",
    "    pickle.dump(all_label_bio, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
