{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43026 43026\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"texts.pkl\",'rb') as f:\n",
    "    texts  = pickle.loads(f.read())\n",
    "with open(\"all_label_bio.pkl\",'rb') as f:\n",
    "    labels  = pickle.loads(f.read())\n",
    "print(len(texts), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jia/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/jia/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "43026it [00:24, 1747.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "from torch.utils.data import DataLoader, TensorDataset  \n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup  \n",
    "from tqdm import tqdm\n",
    "\n",
    "dic = {\"O\":0, \"B-LOC\":1, \"I-LOC\":2}  \n",
    "  \n",
    "# initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('../bert-base-uncased')  \n",
    "  \n",
    "# turn texts into input_ids, attention_masks, label_ids\n",
    "input_ids = []  \n",
    "attention_masks = []  \n",
    "label_ids = []  \n",
    "  \n",
    "max_seq_length = 100 \n",
    "  \n",
    "for text, label in tqdm(zip(texts, labels)):  \n",
    "    encoded_text = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_seq_length)  \n",
    "    input_ids.append(encoded_text['input_ids'].squeeze(0))  \n",
    "    attention_masks.append(encoded_text['attention_mask'].squeeze(0))  \n",
    "      \n",
    "    # turn label into ids \n",
    "    encoded_label = [0]+[dic.get(l, 0) for l in label] \n",
    "    encoded_label += [0] * (max_seq_length - len(encoded_label)) \n",
    "    encoded_label = encoded_label[:100] \n",
    "    encoded_label = torch.tensor(encoded_label, dtype=torch.long)  \n",
    "    label_ids.append(encoded_label)  \n",
    "from torch.utils.data import random_split\n",
    "# turn lists into tensors\n",
    "input_ids = torch.stack(input_ids, dim=0)  \n",
    "attention_masks = torch.stack(attention_masks, dim=0)  \n",
    "label_ids = torch.stack(label_ids, dim=0) \n",
    "\n",
    "# create dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Input IDs: tensor([  101,  1045,  2444,  1999, 19372,  2237,  2803,  1010,  2012,  2028,\n",
      "         2051,  1996,  3007,  1997,  2563,  1006,   999,  1007,  1998,  2073,\n",
      "         2952,  5660,  2288,  2496,   999,  2009,  2036,  2018,  1996,  2922,\n",
      "         5645,  4170,  1999,  2563,  1998,  2028,  1997,  1996,  2922, 15947,\n",
      "         2127,  2332,  2888,  5296,  2009,  2091,  1999,  1996, 14883,  2015,\n",
      "         1006,  1996,  2277,  2145,  3464,  1998,  2064,  2022,  2464,  2013,\n",
      "         2115,  3332,  1007,  1012,  2009,  1005,  1055,  2747, 14996,  1037,\n",
      "         2843,  1997, 15582,  2007,  3488,  2005,  1037,  3518,  2143,  2996,\n",
      "         1010,  4435,  2047,  6023,  2803,  1998,  1037,  2047,  2276,  1012,\n",
      "         2045,  2024,  2048,  9726,  2015,  2306,  3788,  3292,  1010,   102])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "Label IDs: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "Sample 2:\n",
      "Input IDs: tensor([  101,  2057,  2031,  1037,  4310,  5988,  2170,  1996,  6708,  2029,\n",
      "         2003,  4011,  2000,  2022,  1996,  4587, 14678,  2770,  5988,  1999,\n",
      "         1996,  2406,  1012,  2045,  2024,  3518,  6328,  2164,  6358,  3702,\n",
      "         2073,  2112,  1997,  1996,  3185,  1000,  2025,  3436,  7100,  1000,\n",
      "         2001,  6361,  2007,  6423,  7031,  1998,  6621,  3946,  5652,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "Label IDs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "Sample 3:\n",
      "Input IDs: tensor([  101, 18564,  4917,  2380,  2003,  1037,  5379, 13721,  8962,  2451,\n",
      "         3605,  1997,  5037,  1010,  2413,  1010,  3009,  1010,  2690,  2789,\n",
      "         1010,  3493,  1998,  2394,  2945,  1012,  1026,  7987,  1013,  1028,\n",
      "         2057,  2031,  1037,  6919,  3528,  1997,  2248,  7884,  3495,  2104,\n",
      "         2149,  2006,  2358, 21332,  2665,  2346,  1012,  1998,  2045,  2024,\n",
      "         2116,  7340,  1998,  2312,  8915,  9363,  2015, 17006,  2157,  2279,\n",
      "         2341,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
      "         2021,  2017,  2064,  2036,  6957,  2039,  2000, 21676,  2203,  1998,\n",
      "         2247, 15505, 10914,  2073,  2045,  2097, 10866,  3601,  1997,  5037,\n",
      "         1998,  2690,  2789, 12846,  2015,  1012,   102,     0,     0,     0])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0])\n",
      "Label IDs: tensor([0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "Sample 4:\n",
      "Input IDs: tensor([  101,  5647,  1010,  4251,  1998,  2665, 10971,  2007,  2184,  8117,\n",
      "         3328,  2000,  2334,  7884,  1010,  7340,  1998, 19870,  1012,  2017,\n",
      "         2097,  2293,  1996,  3193,  1997,  1996,  9154,  2013,  1996,  4257,\n",
      "         1010,  1019,  8117,  3328,  2000, 21765,  2380,  2043,  2006,  1037,\n",
      "         2204,  1010,  2053,  8044,  2154,  2017,  2097,  2156,  1037, 16538,\n",
      "         3346,  1010,  2059,  2321,  8117,  3328,  2017,  2097,  2131,  2000,\n",
      "         1996,  3267,  3914,  1011,  2026,  3167,  8837,  1010,  5023,  3962,\n",
      "         1997,  2414,   999,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
      "         1028,  2152,  2395,  2003,  2440,  1997,  5636,  2389,  2833,  1011,\n",
      "         6904,  6767, 10179, 19731,  2003, 14469,  5185, 16159,  1024,   102])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "Label IDs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "Sample 5:\n",
      "Input IDs: tensor([  101,  1996, 10971,  2003,  3647,  1998,  2116,  2047, 23812,  1998,\n",
      "         6963,  2024,  6037,  2035,  1996,  2051,  1012,  1045,  2572,  2036,\n",
      "         2200,  2485,  2000,  1996,  3297,  7987,  7646,  2669,  2352,  1998,\n",
      "         6121,  4186,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "Label IDs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 2, 2, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first few samples of the dataset\n",
    "for i in range(5):\n",
    "    input_id = dataset[i][0]\n",
    "    attention_mask = dataset[i][1]\n",
    "    label_id = dataset[i][2]\n",
    "    \n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(\"Input IDs:\", input_id)\n",
    "    print(\"Attention Mask:\", attention_mask)\n",
    "    print(\"Label IDs:\", label_id)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34420 8606\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "# Create data loaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jia/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# load the pre-trained model\n",
    "model = BertForTokenClassification.from_pretrained('../bert-base-uncased', num_labels=3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# define the optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "# define the scheduler\n",
    "epochs = 3\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [02:29<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2260\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7393    0.1290    0.2197    584755\n",
      "           1     0.0270    0.3775    0.0504     32399\n",
      "           2     0.0243    0.0692    0.0360     33958\n",
      "\n",
      "    accuracy                         0.1383    651112\n",
      "   macro avg     0.2636    0.1919    0.1020    651112\n",
      "weighted avg     0.6666    0.1383    0.2017    651112\n",
      "\n",
      "test loss before train: 1.2259918451309204\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F  \n",
    "\n",
    "def test():\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            batch_predictions = outputs.logits\n",
    "            # catch the predicted ids\n",
    "            predicted_ids = torch.argmax(F.log_softmax(batch_predictions, dim=2), dim=2) \n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Only keep predictions for non-padding tokens\n",
    "            for i, mask in enumerate(b_input_mask):\n",
    "                true_len = torch.sum(mask).item()\n",
    "                all_labels.extend(b_labels[i, :true_len].cpu().numpy())\n",
    "                all_preds.extend(predicted_ids[i, :true_len].cpu().numpy())\n",
    "                # print(len(all_labels), len(all_preds))\n",
    "\n",
    "    print(f'Test Loss: {loss.item():.4f}')\n",
    "    # Compute evaluation metrics\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "    print(report)\n",
    "    return loss.item()\n",
    "\n",
    "best_loss = test()\n",
    "print('test loss before train:', best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/1076], Loss: 0.0667\n",
      "Epoch [1/3], Step [200/1076], Loss: 0.0668\n",
      "Epoch [1/3], Step [300/1076], Loss: 0.0349\n",
      "Epoch [1/3], Step [400/1076], Loss: 0.0400\n",
      "Epoch [1/3], Step [500/1076], Loss: 0.0257\n",
      "Epoch [1/3], Step [600/1076], Loss: 0.0406\n",
      "Epoch [1/3], Step [700/1076], Loss: 0.0647\n",
      "Epoch [1/3], Step [800/1076], Loss: 0.0382\n",
      "Epoch [1/3], Step [900/1076], Loss: 0.0345\n",
      "Epoch [1/3], Step [1000/1076], Loss: 0.0309\n",
      "Epoch [1/3], Loss: 0.0360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [02:35<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0320\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9942    0.9894    0.9918    584755\n",
      "           1     0.9231    0.9482    0.9355     32399\n",
      "           2     0.8874    0.9387    0.9123     33958\n",
      "\n",
      "    accuracy                         0.9847    651112\n",
      "   macro avg     0.9349    0.9588    0.9465    651112\n",
      "weighted avg     0.9851    0.9847    0.9849    651112\n",
      "\n",
      "save model\n",
      "Epoch [2/3], Step [100/1076], Loss: 0.0221\n",
      "Epoch [2/3], Step [200/1076], Loss: 0.0520\n",
      "Epoch [2/3], Step [300/1076], Loss: 0.0276\n",
      "Epoch [2/3], Step [400/1076], Loss: 0.0262\n",
      "Epoch [2/3], Step [500/1076], Loss: 0.0320\n",
      "Epoch [2/3], Step [600/1076], Loss: 0.0290\n",
      "Epoch [2/3], Step [700/1076], Loss: 0.0144\n",
      "Epoch [2/3], Step [800/1076], Loss: 0.0284\n",
      "Epoch [2/3], Step [900/1076], Loss: 0.0313\n",
      "Epoch [2/3], Step [1000/1076], Loss: 0.0165\n",
      "Epoch [2/3], Loss: 0.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [02:36<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9941    0.9921    0.9931    584755\n",
      "           1     0.9426    0.9483    0.9455     32399\n",
      "           2     0.9129    0.9386    0.9256     33958\n",
      "\n",
      "    accuracy                         0.9871    651112\n",
      "   macro avg     0.9499    0.9597    0.9547    651112\n",
      "weighted avg     0.9873    0.9871    0.9872    651112\n",
      "\n",
      "save model\n",
      "Epoch [3/3], Step [100/1076], Loss: 0.0182\n",
      "Epoch [3/3], Step [200/1076], Loss: 0.0144\n",
      "Epoch [3/3], Step [300/1076], Loss: 0.0161\n",
      "Epoch [3/3], Step [400/1076], Loss: 0.0103\n",
      "Epoch [3/3], Step [500/1076], Loss: 0.0166\n",
      "Epoch [3/3], Step [600/1076], Loss: 0.0145\n",
      "Epoch [3/3], Step [700/1076], Loss: 0.0251\n",
      "Epoch [3/3], Step [800/1076], Loss: 0.0231\n",
      "Epoch [3/3], Step [900/1076], Loss: 0.0096\n",
      "Epoch [3/3], Step [1000/1076], Loss: 0.0195\n",
      "Epoch [3/3], Loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [02:58<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9953    0.9915    0.9934    584755\n",
      "           1     0.9409    0.9564    0.9486     32399\n",
      "           2     0.9073    0.9518    0.9290     33958\n",
      "\n",
      "    accuracy                         0.9877    651112\n",
      "   macro avg     0.9478    0.9666    0.9570    651112\n",
      "weighted avg     0.9880    0.9877    0.9878    651112\n",
      "\n",
      "Best loss: 0.03171028941869736\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Training loop for one epoch\n",
    "    for batch_idx, batch in enumerate(train_data_loader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    now_test_loss = test()\n",
    "    # Check if current loss is the best\n",
    "    if now_test_loss < best_loss:\n",
    "        best_loss = now_test_loss\n",
    "        # save the best model\n",
    "        print('save model')\n",
    "        torch.save(model.state_dict(), 'bert_ner_model.pt')\n",
    "        \n",
    "    model.train()\n",
    "\n",
    "print('Best loss:', best_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
